{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMV6V9hW8q275zsAJkBc0r2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-ML-Zoomcamp-Week-5/blob/main/ML_Zoomcamp_Week_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goldsmiths University of London\n",
        "**Author....: Carlos Manuel de Oliveira Alves**<br>\n",
        "**Student..: cdeol003**<br>\n",
        "**Created..: 03/10/2022**"
      ],
      "metadata": {
        "id": "i6Ma_jB9wsko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fn6xjOAtwp3M"
      },
      "outputs": [],
      "source": [
        "# Import all necessay libraries that we will use in this project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Import the library warnings to ignore the warnings from the system\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation:\n",
        "\n",
        "# Read the dataset, store it in dataframe\n",
        "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "# Make the categorical data of the dataframe consistent \n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
        "\n",
        "for col in categorical_columns:\n",
        "  df[col] = df[col].str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Convert a serie of the dataframe to a number\n",
        "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
        "\n",
        "# Fill the missing values of the total charges serie with zeros\n",
        "df.totalcharges = df.totalcharges.fillna(0)\n",
        "\n",
        "# Update the churn data with numbers\n",
        "df.churn = (df.churn == 'yes').astype(int)"
      ],
      "metadata": {
        "id": "ZMwYnw4m0KUv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sizes of the datasets with 20% and use random state so the results are reproducible\n",
        "# the full train dataset has 80% and the test 20%\n",
        "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "QxcW7Vdz3fE1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list with numerical variables from the dataframe\n",
        "numerical = ['tenure','monthlycharges','totalcharges']\n",
        "\n",
        "# Create a list with categorical variables from the dataframe\n",
        "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
        "    'phoneservice', 'multiplelines', 'internetservice',\n",
        "    'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
        "    'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
        "    'paymentmethod']"
      ],
      "metadata": {
        "id": "ENEJ1fej32YK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for training\n",
        "def train(data_train, y_train, C=1.0):\n",
        "    \n",
        "    # Create dictionaries that extract from our dataframe categoricl and numerical variables\n",
        "    dicts = data_train[categorical+numerical].to_dict(orient='records')\n",
        "\n",
        "    # Create a new instance of the dictionary vectorizer class without sparse\n",
        "    dv = DictVectorizer(sparse=False)\n",
        "    \n",
        "    # Use the function transform with our dictionary vectorizer\n",
        "    X_train = dv.fit_transform(dicts)\n",
        "\n",
        "    # Create a model logistic regression and define the parameter and duration\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    \n",
        "    # For training the model we use the fit method\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Return the DictVectorizer and model\n",
        "    return dv, model"
      ],
      "metadata": {
        "id": "uvJFRQbP4RUL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for predict\n",
        "def predict(data, dv, model):\n",
        "\n",
        "    # Convert the dataframe into a list of dictionaries\n",
        "    dicts = data[categorical+numerical].to_dict(orient='records')\n",
        "\n",
        "    # Creates the feature matrix using the vectorizer\n",
        "    X = dv.transform(dicts)\n",
        "\n",
        "    # Use the model predict proba and take the second column\n",
        "    y_pred = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Return our prediction\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "6hY0YelL4vGR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters of the model\n",
        "C = 1.0\n",
        "n_splits = 5"
      ],
      "metadata": {
        "id": "_MYGpDQr46lV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the function KFold to split the data in 5 parts and seed 1\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "\n",
        "# Create a list with scores\n",
        "scores = []\n",
        "\n",
        "# Iterate over the full train dataframe using the function split together with train and val indexes\n",
        "for train_idx, val_idx in kfold.split(df_full_train):\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for train\n",
        "    df_train = df_full_train.iloc[train_idx]\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for validation\n",
        "    df_val = df_full_train.iloc[val_idx]\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for train and validation\n",
        "    y_train = df_train.churn.values\n",
        "    y_val = df_val.churn.values\n",
        "\n",
        "    # Call function train and store results of dv and model\n",
        "    dv, model = train(df_train, y_train,C=1)\n",
        "\n",
        "    # Call the function predict and use with our validation datset\n",
        "    y_pred = predict(df_val, dv, model)\n",
        "    \n",
        "    # Compute and store the ROC AUC score\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "    \n",
        "    # After evaluate the model we store the results\n",
        "    scores.append(auc)\n",
        "\n",
        "# Print the mean score and standard deviation\n",
        "print(f'C={C} {np.mean(scores):.3f} +- {np.std(scores):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-mSk0_G5I3Q",
        "outputId": "132421c1-54ef-49b5-c415-b06b26a267fd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.0 0.841 +- 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the array scores\n",
        "scores"
      ],
      "metadata": {
        "id": "2UCqLYgu5_Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ad1ee3-aa03-4337-c87c-838f9ad2a1ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8424142992833088,\n",
              " 0.8455854357038802,\n",
              " 0.8325627132249649,\n",
              " 0.8301684306452645,\n",
              " 0.8522128267076653]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train our final model:\n",
        "\n",
        "# Call function train and store results of dv and model\n",
        "dv, model = train(df_full_train, df_full_train.churn.values,C=1)\n",
        "\n",
        "# Call the function predict and use with our test datset\n",
        "y_pred = predict(df_test, dv, model)\n",
        "\n",
        "# Get the card variable from validation dataframe\n",
        "y_test = df_test.churn.values\n",
        "\n",
        "# Compute and store the ROC AUC score\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23iR811B6OMe",
        "outputId": "14807a05-37a1-4d34-b2d3-1292d71bc1e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8572386167896259"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model:\n",
        "\n",
        "# to save the model we use pickle for saving python objects\n",
        "# Import pickle library or saving python objects\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OB1_ZpeY8dFD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take our model and write it to a file\n",
        "\n",
        "# Create the filename to store our model\n",
        "output_file = f'model_C={C}.bin'\n",
        "output_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "skNOtXM99IKV",
        "outputId": "65a4c8bb-2235-4f81-fff4-478745154b1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_C=1.0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a file with the filename created and write the file\n",
        "f_out = open(output_file, 'wb')\n",
        "# wb means write a binaray file\n",
        "\n",
        "# Use pickle to save the dictionary vectorizer and our model\n",
        "pickle.dump((dv, model), f_out)\n",
        "\n",
        "# Close the file created\n",
        "f_out.close()"
      ],
      "metadata": {
        "id": "f7u80a3s9krm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the with statement to make sure the file is closed\n",
        "with open(output_file, 'wb') as f_out:\n",
        "  pickle.dump((dv, model), f_out)\n",
        "# as we are out of the statement the file is automaticaly closed"
      ],
      "metadata": {
        "id": "DRSapoVfASP8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model:\n",
        "\n",
        "# Import pickle library to access the file created\n",
        "import pickle\n",
        "\n",
        "# Define the name of the file we want to open\n",
        "model_file = 'model_C=1.0.bin'\n",
        "\n",
        "# Open the model using the with statement\n",
        "with open(model_file, 'rb') as f_in:\n",
        "  dv, model = pickle.load(f_in)\n",
        "# rb stands for reading binary file"
      ],
      "metadata": {
        "id": "Z46GM8LzA6lw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check we have access to our dictionary vectorizer and model\n",
        "dv, model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlSQHoI4CXUR",
        "outputId": "a2085e35-8392-4e93-eceb-b312115802c0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DictVectorizer(sparse=False), LogisticRegression(C=1, max_iter=1000))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create customer dictionary to be used with our model\n",
        "customer = {\n",
        "    'gender' : 'female',\n",
        "    'seniorcitizen': 0,\n",
        "    'partner': 'yes',\n",
        "    'dependents': 'no',\n",
        "    'phoneservice': 'no',\n",
        "    'multiplelines': 'no_phone_service',\n",
        "    'internetservice': 'dsl',\n",
        "    'onlinesecurity': 'no',\n",
        "    'onlinebackup': 'yes',\n",
        "    'deviceprotection': 'no',\n",
        "    'techsuport': 'no',\n",
        "    'streamingtv': 'no',\n",
        "    'contract': 'month-to-month',\n",
        "    'paperlessbilling': 'yes',\n",
        "    'paymentmethod': 'electronic_check',\n",
        "    'tenure': 1,\n",
        "    'monthlycharges': 29.85,\n",
        "    'totalcharges': 29.85\n",
        "}"
      ],
      "metadata": {
        "id": "1Y5B6MG5C8Qr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Churn this customer into feature matrix using our dictionary vectorizer transform method\n",
        "X = dv.transform([customer])\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrvA-ICKH_E",
        "outputId": "b327b11b-36db-43de-bfc7-a4832990d73a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  1.  ,\n",
              "         0.  ,  1.  ,  0.  ,  0.  , 29.85,  0.  ,  1.  ,  0.  ,  0.  ,\n",
              "         0.  ,  1.  ,  1.  ,  0.  ,  0.  ,  0.  ,  1.  ,  0.  ,  1.  ,\n",
              "         0.  ,  0.  ,  1.  ,  0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
              "         0.  ,  1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  1.  , 29.85]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call model using the method predict proba with X\n",
        "model.predict_proba(X)\n",
        "# it generates a two-dimensional array and we are interested in the second element"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yj6ggdZKsGv",
        "outputId": "832ef7e4-c7fe-40eb-d0a3-ad58b6025a6e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39956675, 0.60043325]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call model using the method predict proba with X and get the probability\n",
        "# that this partcular customer is going to churn\n",
        "model.predict_proba(X)[0, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWZxQJ6fLCv1",
        "outputId": "f6fde128-1bbc-4f9b-ffb0-23affcb8460c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.600433253756712"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_jfxTpH3Lhaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOWIQDaQwzlhT9whS83/HwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carlos-alves-one/-ML-Zoomcamp-Week-5/blob/main/ML_Zoomcamp_Week_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Goldsmiths University of London\n",
        "**Author....: Carlos Manuel de Oliveira Alves**<br>\n",
        "**Student..: cdeol003**<br>\n",
        "**Created..: 03/10/2022**"
      ],
      "metadata": {
        "id": "i6Ma_jB9wsko"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn6xjOAtwp3M"
      },
      "outputs": [],
      "source": [
        "# Import all necessay libraries that we will use in this project\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Import the library warnings to ignore the warnings from the system\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preparation:\n",
        "\n",
        "# Read the dataset, store it in dataframe\n",
        "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "# Make the categorical data of the dataframe consistent \n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n",
        "\n",
        "for col in categorical_columns:\n",
        "  df[col] = df[col].str.lower().str.replace(' ', '_')\n",
        "\n",
        "# Convert a serie of the dataframe to a number\n",
        "df.totalcharges = pd.to_numeric(df.totalcharges, errors='coerce')\n",
        "\n",
        "# Fill the missing values of the total charges serie with zeros\n",
        "df.totalcharges = df.totalcharges.fillna(0)\n",
        "\n",
        "# Update the churn data with numbers\n",
        "df.churn = (df.churn == 'yes').astype(int)"
      ],
      "metadata": {
        "id": "ZMwYnw4m0KUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sizes of the datasets with 20% and use random state so the results are reproducible\n",
        "# the full train dataset has 80% and the test 20%\n",
        "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "QxcW7Vdz3fE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list with numerical variables from the dataframe\n",
        "numerical = ['tenure','monthlycharges','totalcharges']\n",
        "\n",
        "# Create a list with categorical variables from the dataframe\n",
        "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
        "    'phoneservice', 'multiplelines', 'internetservice',\n",
        "    'onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport',\n",
        "    'streamingtv', 'streamingmovies', 'contract', 'paperlessbilling',\n",
        "    'paymentmethod']"
      ],
      "metadata": {
        "id": "ENEJ1fej32YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for training\n",
        "def train(data_train, y_train, C=1.0):\n",
        "    \n",
        "    # Create dictionaries that extract from our dataframe categoricl and numerical variables\n",
        "    dicts = data_train[categorical+numerical].to_dict(orient='records')\n",
        "\n",
        "    # Create a new instance of the dictionary vectorizer class without sparse\n",
        "    dv = DictVectorizer(sparse=False)\n",
        "    \n",
        "    # Use the function transform with our dictionary vectorizer\n",
        "    X_train = dv.fit_transform(dicts)\n",
        "\n",
        "    # Create a model logistic regression and define the parameter and duration\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    \n",
        "    # For training the model we use the fit method\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Return the DictVectorizer and model\n",
        "    return dv, model"
      ],
      "metadata": {
        "id": "uvJFRQbP4RUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function for predict\n",
        "def predict(data, dv, model):\n",
        "\n",
        "    # Convert the dataframe into a list of dictionaries\n",
        "    dicts = data[categorical+numerical].to_dict(orient='records')\n",
        "\n",
        "    # Creates the feature matrix using the vectorizer\n",
        "    X = dv.transform(dicts)\n",
        "\n",
        "    # Use the model predict proba and take the second column\n",
        "    y_pred = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Return our prediction\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "6hY0YelL4vGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters of the model\n",
        "C = 1.0\n",
        "n_splits = 5"
      ],
      "metadata": {
        "id": "_MYGpDQr46lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the function KFold to split the data in 5 parts and seed 1\n",
        "kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
        "\n",
        "# Create a list with scores\n",
        "scores = []\n",
        "\n",
        "# Iterate over the full train dataframe using the function split together with train and val indexes\n",
        "for train_idx, val_idx in kfold.split(df_full_train):\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for train\n",
        "    df_train = df_full_train.iloc[train_idx]\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for validation\n",
        "    df_val = df_full_train.iloc[val_idx]\n",
        "\n",
        "    # Use iloc to select a part of the full train dataframe for train and validation\n",
        "    y_train = df_train.churn.values\n",
        "    y_val = df_val.churn.values\n",
        "\n",
        "    # Call function train and store results of dv and model\n",
        "    dv, model = train(df_train, y_train,C=1)\n",
        "\n",
        "    # Call the function predict and use with our validation datset\n",
        "    y_pred = predict(df_val, dv, model)\n",
        "    \n",
        "    # Compute and store the ROC AUC score\n",
        "    auc = roc_auc_score(y_val, y_pred)\n",
        "    \n",
        "    # After evaluate the model we store the results\n",
        "    scores.append(auc)\n",
        "\n",
        "# Print the mean score and standard deviation\n",
        "print(f'C={C} {np.mean(scores):.3f} +- {np.std(scores):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-mSk0_G5I3Q",
        "outputId": "72381262-0585-452d-de17-4b9de12a852d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=1.0 0.840 +- 0.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the array scores\n",
        "scores"
      ],
      "metadata": {
        "id": "2UCqLYgu5_Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f1ef39-f333-4f52-ce35-32066aa0c217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8419943324096679,\n",
              " 0.8455854357038802,\n",
              " 0.8311739915713425,\n",
              " 0.8301684306452645,\n",
              " 0.851750023532365]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train our final model:\n",
        "\n",
        "# Call function train and store results of dv and model\n",
        "dv, model = train(df_full_train, df_full_train.churn.values,C=1)\n",
        "\n",
        "# Call the function predict and use with our test datset\n",
        "y_pred = predict(df_test, dv, model)\n",
        "\n",
        "# Get the card variable from validation dataframe\n",
        "y_test = df_test.churn.values\n",
        "\n",
        "# Compute and store the ROC AUC score\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23iR811B6OMe",
        "outputId": "53b4f756-9063-46be-b909-d85d382103e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8572386167896259"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model:\n",
        "\n",
        "# to save the model we use pickle for saving python objects\n",
        "# Import pickle library or saving python objects\n",
        "import pickle"
      ],
      "metadata": {
        "id": "OB1_ZpeY8dFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take our model and write it to a file\n",
        "\n",
        "# Create the filename to store our model\n",
        "output_file = f'model_C={C}.bin'\n",
        "output_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "skNOtXM99IKV",
        "outputId": "ed8b8662-deb5-4c21-ac42-e96f72e12c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model_C=1.0.bin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a file with the filename created and write the file\n",
        "f_out = open(output_file, 'wb')\n",
        "# wb means write a binaray file\n",
        "\n",
        "# Use pickle to save the dictionary vectorizer and our model\n",
        "pickle.dump((dv, model), f_out)\n",
        "\n",
        "# Close the file created\n",
        "f_out.close()"
      ],
      "metadata": {
        "id": "f7u80a3s9krm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the with statement to make sure the file is closed\n",
        "with open(output_file, 'wb') as f_out:\n",
        "  pickle.dump((dv, model), f_out)\n",
        "# as we are out of the statement the file is automaticaly closed"
      ],
      "metadata": {
        "id": "DRSapoVfASP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model:\n",
        "\n",
        "# Import pickle library to access the file created\n",
        "import pickle\n",
        "\n",
        "# Define the name of the file we want to open\n",
        "model_file = 'model_C=1.0.bin'\n",
        "\n",
        "# Open the model using the with statement\n",
        "with open(model_file, 'rb') as f_in:\n",
        "  dv, model = pickle.load(f_in)\n",
        "# rb stands for reading binary file"
      ],
      "metadata": {
        "id": "Z46GM8LzA6lw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "236445df-cc84-4338-c615-844ba1a26cd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd8347e138d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Open the model using the with statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mdv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# rb stands for reading binary file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_C=1.0.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check we have access to our dictionary vectorizer and model\n",
        "dv, model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlSQHoI4CXUR",
        "outputId": "594cf033-a01b-4d56-efd1-1c8e81d83794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DictVectorizer(sparse=False), LogisticRegression(C=1, max_iter=1000))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create customer dictionary to be used with our model\n",
        "customer = {\n",
        "    'gender' : 'female',\n",
        "    'seniorcitizen': 0,\n",
        "    'partner': 'yes',\n",
        "    'dependents': 'no',\n",
        "    'phoneservice': 'no',\n",
        "    'multiplelines': 'no_phone_service',\n",
        "    'internetservice': 'dsl',\n",
        "    'onlinesecurity': 'no',\n",
        "    'onlinebackup': 'yes',\n",
        "    'deviceprotection': 'no',\n",
        "    'techsuport': 'no',\n",
        "    'streamingtv': 'no',\n",
        "    'contract': 'month-to-month',\n",
        "    'paperlessbilling': 'yes',\n",
        "    'paymentmethod': 'electronic_check',\n",
        "    'tenure': 1,\n",
        "    'monthlycharges': 29.85,\n",
        "    'totalcharges': 29.85\n",
        "}"
      ],
      "metadata": {
        "id": "1Y5B6MG5C8Qr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fc57daa4-cff9-43a7-9f27-9fa6d52cebb8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'contract'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}